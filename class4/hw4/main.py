"""
FastAPI Service for arXiv RAG System
======================================
This service provides a REST API endpoint for searching arXiv papers
using the FAISS index and embeddings generated by hw4.py

Endpoints:
- GET /: API information
- GET /search?q={query}&k={num_results}: Search for relevant passages
- GET /health: Health check
"""

import os
import warnings

# Suppress multiprocessing warnings
warnings.filterwarnings("ignore", category=UserWarning, module="resource_tracker")
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# Disable PyTorch multiprocessing to prevent segfaults on Python 3.13
import torch
torch.set_num_threads(1)

import pickle
from pathlib import Path
from typing import List, Dict, Optional

import faiss
import numpy as np
from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer, CrossEncoder
import uvicorn


# Response models
class SearchResult(BaseModel):
    """Model for a single search result"""
    rank: int
    chunk: str
    paper_title: str
    paper_id: str
    chunk_id: int
    total_chunks: int
    similarity: float


class SearchResponse(BaseModel):
    """Model for search response"""
    query: str
    num_results: int
    results: List[SearchResult]


class HealthResponse(BaseModel):
    """Model for health check response"""
    status: str
    index_size: int
    model: str


class APIInfo(BaseModel):
    """Model for API information"""
    name: str
    version: str
    description: str
    endpoints: Dict[str, str]


# Initialize FastAPI app
app = FastAPI(
    title="arXiv RAG Search API",
    description="Search arXiv cs.CL papers using semantic search",
    version="1.0.0"
)


# Global variables for loaded resources
faiss_index = None
chunks = None
chunk_metadata = None
model = None
model_name = None
cross_encoder = None  # Lazy-loaded for reranking


def load_resources(data_dir: str = "./arxiv_data"):
    """
    Load FAISS index, chunks, and embedding model.

    Args:
        data_dir: Directory containing the generated files
    """
    global faiss_index, chunks, chunk_metadata, model, model_name

    data_path = Path(data_dir)

    print("Loading resources...")

    # Load FAISS index
    index_path = data_path / "faiss_index.bin"
    if not index_path.exists():
        raise FileNotFoundError(f"FAISS index not found at {index_path}")
    faiss_index = faiss.read_index(str(index_path))
    print(f"✓ Loaded FAISS index: {faiss_index.ntotal} vectors")

    # Load chunks and metadata
    api_data_path = data_path / "api_data.pkl"
    if not api_data_path.exists():
        raise FileNotFoundError(f"API data not found at {api_data_path}")

    with open(api_data_path, 'rb') as f:
        api_data = pickle.load(f)

    chunks = api_data['chunks']
    chunk_metadata = api_data['chunk_metadata']
    model_name = api_data['model_name']
    print(f"✓ Loaded {len(chunks)} chunks")

    # Load embedding model
    model = SentenceTransformer(model_name)
    print(f"✓ Loaded embedding model: {model_name}")

    print("All resources loaded successfully!")


# Load resources on startup
@app.on_event("startup")
async def startup_event():
    """Load resources when the API starts"""
    try:
        load_resources()
    except Exception as e:
        print(f"Error loading resources: {e}")
        print("Please run hw4.py first to generate the necessary files.")
        raise


@app.get("/", response_model=APIInfo)
async def root():
    """
    Get API information and available endpoints.
    """
    return APIInfo(
        name="arXiv RAG Search API",
        version="1.0.0",
        description="Semantic search over arXiv cs.CL papers using FAISS and sentence transformers",
        endpoints={
            "/": "API information",
            "/search": "Search for relevant passages (params: q=query, k=num_results)",
            "/search/rerank": "Search with cross-encoder reranking (params: q=query, k=num_results, initial_k=candidates)",
            "/search/metadata": "Search with metadata filtering and boosting (params: q=query, k=num_results, year_min, year_max, authors, recency_boost)",
            "/health": "Health check",
            "/stats": "System statistics",
            "/docs": "Interactive API documentation"
        }
    )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """
    Health check endpoint.
    """
    if faiss_index is None:
        raise HTTPException(status_code=503, detail="Resources not loaded")

    return HealthResponse(
        status="healthy",
        index_size=faiss_index.ntotal,
        model=model_name
    )


@app.get("/search", response_model=SearchResponse)
async def search(
    q: str = Query(..., description="Search query", min_length=1),
    k: int = Query(3, description="Number of results to return", ge=1, le=10)
):
    """
    Search for top-k most relevant passages.

    Args:
        q: Search query string
        k: Number of results to return (1-10)

    Returns:
        SearchResponse with ranked results
    """
    if faiss_index is None or model is None:
        raise HTTPException(status_code=503, detail="Resources not loaded")

    try:
        # Embed the query (normalize for cosine similarity)
        query_embedding = model.encode([q], convert_to_numpy=True, normalize_embeddings=True)

        # Search FAISS index (returns cosine similarities with IndexFlatIP)
        similarities, indices = faiss_index.search(query_embedding.astype('float32'), k)

        # Format results
        results = []
        for rank, (idx, similarity) in enumerate(zip(indices[0], similarities[0]), 1):
            # Cosine similarity (higher = more similar, range -1 to 1)
            similarity_score = float(similarity)

            # Get chunk and metadata
            chunk_text = chunks[idx]
            metadata = chunk_metadata[idx]

            results.append(SearchResult(
                rank=rank,
                chunk=chunk_text,
                paper_title=metadata['paper_title'],
                paper_id=metadata['paper_id'],
                chunk_id=metadata['chunk_id'],
                total_chunks=metadata['total_chunks'],
                similarity=similarity_score
            ))

        return SearchResponse(
            query=q,
            num_results=len(results),
            results=results
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search error: {str(e)}")


@app.get("/search/rerank", response_model=SearchResponse)
async def search_with_rerank(
    q: str = Query(..., description="Search query", min_length=1),
    k: int = Query(3, description="Number of final results to return", ge=1, le=10),
    initial_k: int = Query(20, description="Number of candidates to retrieve before reranking", ge=10, le=50)
):
    """
    Search for top-k most relevant passages with cross-encoder reranking.

    This implements a two-stage retrieval:
    1. First stage: Use FAISS to get initial_k candidates (fast, approximate)
    2. Second stage: Use cross-encoder to rerank and select top k (slow, accurate)

    This generally provides better precision than the basic /search endpoint.

    Args:
        q: Search query string
        k: Number of final results to return (1-10)
        initial_k: Number of candidates before reranking (10-50, should be > k)

    Returns:
        SearchResponse with reranked results
    """
    global cross_encoder

    if faiss_index is None or model is None:
        raise HTTPException(status_code=503, detail="Resources not loaded")

    try:
        # Stage 1: Fast retrieval with FAISS (normalize for cosine similarity)
        query_embedding = model.encode([q], convert_to_numpy=True, normalize_embeddings=True)
        similarities, indices = faiss_index.search(query_embedding.astype('float32'), initial_k)

        # Collect candidates
        candidates = []
        for idx in indices[0]:
            candidates.append({
                "idx": idx,
                "chunk": chunks[idx],
                "metadata": chunk_metadata[idx]
            })

        # Stage 2: Rerank with cross-encoder
        # Lazy load cross-encoder
        if cross_encoder is None:
            print("Loading cross-encoder model for reranking...")
            cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

        # Prepare query-document pairs
        query_doc_pairs = [[q, candidate["chunk"]] for candidate in candidates]

        # Get cross-encoder scores
        ce_scores = cross_encoder.predict(query_doc_pairs)

        # Add scores and sort
        for candidate, score in zip(candidates, ce_scores):
            candidate["ce_score"] = float(score)

        # Sort by cross-encoder score (higher is better)
        candidates.sort(key=lambda x: x["ce_score"], reverse=True)

        # Format top-k results
        results = []
        for rank, candidate in enumerate(candidates[:k], 1):
            metadata = candidate["metadata"]
            results.append(SearchResult(
                rank=rank,
                chunk=candidate["chunk"],
                paper_title=metadata['paper_title'],
                paper_id=metadata['paper_id'],
                chunk_id=metadata['chunk_id'],
                total_chunks=metadata['total_chunks'],
                similarity=candidate["ce_score"]  # Use cross-encoder score
            ))

        return SearchResponse(
            query=q,
            num_results=len(results),
            results=results
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Reranking error: {str(e)}")


@app.get("/search/metadata", response_model=SearchResponse)
async def search_with_metadata(
    q: str = Query(..., description="Search query", min_length=1),
    k: int = Query(3, description="Number of final results to return", ge=1, le=10),
    initial_k: int = Query(50, description="Number of candidates before filtering", ge=10, le=100),
    year_min: Optional[int] = Query(None, description="Minimum publication year (inclusive)", ge=1990, le=2025),
    year_max: Optional[int] = Query(None, description="Maximum publication year (inclusive)", ge=1990, le=2025),
    authors: Optional[str] = Query(None, description="Comma-separated list of author names to filter by"),
    recency_boost: float = Query(0.0, description="Recency boost factor (0.0-1.0)", ge=0.0, le=1.0)
):
    """
    Search with metadata filtering and score boosting.

    This endpoint allows you to:
    1. Filter results by publication year range
    2. Filter results by author names (partial matching)
    3. Boost scores for more recent papers

    Args:
        q: Search query string
        k: Number of final results to return (1-10)
        initial_k: Number of candidates before filtering (10-100)
        year_min: Minimum publication year (e.g., 2020)
        year_max: Maximum publication year (e.g., 2024)
        authors: Comma-separated author names (e.g., "Vaswani,Devlin")
        recency_boost: Boost factor for recent papers (0.0=none, 0.5=moderate, 1.0=strong)

    Returns:
        SearchResponse with metadata-filtered and boosted results

    Examples:
        - Recent papers: /search/metadata?q=transformers&year_min=2020&recency_boost=0.3
        - Specific author: /search/metadata?q=attention&authors=Vaswani
        - Combined: /search/metadata?q=BERT&year_min=2018&year_max=2020&authors=Devlin
    """
    if faiss_index is None or model is None:
        raise HTTPException(status_code=503, detail="Resources not loaded")

    try:
        # Parse authors list
        author_list = None
        if authors:
            author_list = [a.strip() for a in authors.split(',') if a.strip()]

        # Retrieve candidates with metadata filtering
        query_embedding = model.encode([q], convert_to_numpy=True, normalize_embeddings=True)
        similarities, indices = faiss_index.search(query_embedding.astype('float32'), initial_k)

        # Collect and filter candidates
        candidates = []
        for idx, similarity in zip(indices[0], similarities[0]):
            metadata = chunk_metadata[idx]

            # Apply year filters
            if year_min and metadata.get('published_year'):
                if metadata['published_year'] < year_min:
                    continue

            if year_max and metadata.get('published_year'):
                if metadata['published_year'] > year_max:
                    continue

            # Apply author filter
            if author_list:
                paper_authors = metadata.get('authors', [])
                author_match = False
                for filter_author in author_list:
                    for paper_author in paper_authors:
                        if filter_author.lower() in paper_author.lower():
                            author_match = True
                            break
                    if author_match:
                        break

                if not author_match:
                    continue

            # Base similarity score
            base_score = float(similarity)

            # Apply recency boost
            if recency_boost > 0 and metadata.get('published_year'):
                year = metadata['published_year']
                year_normalized = (year - 2010) / (2024 - 2010)
                year_normalized = max(0, min(1, year_normalized))
                boosted_score = base_score + (recency_boost * year_normalized)
            else:
                boosted_score = base_score

            candidates.append({
                "idx": idx,
                "chunk": chunks[idx],
                "metadata": metadata,
                "similarity": boosted_score
            })

        # Sort by boosted similarity
        candidates.sort(key=lambda x: x["similarity"], reverse=True)

        # Format top-k results
        results = []
        for rank, candidate in enumerate(candidates[:k], 1):
            metadata = candidate["metadata"]
            results.append(SearchResult(
                rank=rank,
                chunk=candidate["chunk"],
                paper_title=metadata['paper_title'],
                paper_id=metadata['paper_id'],
                chunk_id=metadata['chunk_id'],
                total_chunks=metadata['total_chunks'],
                similarity=candidate["similarity"]
            ))

        return SearchResponse(
            query=q,
            num_results=len(results),
            results=results
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Metadata search error: {str(e)}")


@app.get("/stats")
async def get_stats():
    """
    Get statistics about the indexed data.
    """
    if chunks is None or faiss_index is None:
        raise HTTPException(status_code=503, detail="Resources not loaded")

    # Count unique papers
    unique_papers = len(set(meta['paper_id'] for meta in chunk_metadata))

    return {
        "total_chunks": len(chunks),
        "total_papers": unique_papers,
        "index_size": faiss_index.ntotal,
        "embedding_dimension": faiss_index.d,
        "model": model_name,
        "avg_chunks_per_paper": len(chunks) / unique_papers if unique_papers > 0 else 0
    }


# Error handlers
@app.exception_handler(404)
async def not_found_handler(request, exc):
    return JSONResponse(
        status_code=404,
        content={"error": "Endpoint not found", "message": str(exc)}
    )


@app.exception_handler(500)
async def internal_error_handler(request, exc):
    return JSONResponse(
        status_code=500,
        content={"error": "Internal server error", "message": str(exc)}
    )


if __name__ == "__main__":

    print("""
    ================================================================================
    arXiv RAG Search API
    ================================================================================
    Starting FastAPI server...

    API Documentation will be available at:
    - Swagger UI: http://localhost:8000/docs
    - ReDoc: http://localhost:8000/redoc

    Example usage:
    - Search: http://localhost:8000/search?q=transformer%20models&k=3
    - Health: http://localhost:8000/health
    - Stats: http://localhost:8000/stats
    ================================================================================
    """)

    uvicorn.run(app, host="0.0.0.0", port=8000)
